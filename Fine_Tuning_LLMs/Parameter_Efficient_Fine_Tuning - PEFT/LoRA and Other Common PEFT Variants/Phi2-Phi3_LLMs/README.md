# Phi2 and Phi3 LLM Models Fine Tuning

## Phi-3
* Original technical report: https://arxiv.org/abs/2404.14219
* 3.8 billion param LLM
* Performance rivals Mistral 8x7B and GPT-3.5
* 69% on MMLU benchmark
* Main innovation is the dataset they used for training --> was a “scaled up” version of the data used for phi2 which contained filtered public web and synthetic data.
* Phi3 vision also introduced with 4.2 billion parameters for image and text prompts. 
