{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d9e36f-fdf8-48e1-ac05-85dae1e78499",
   "metadata": {},
   "source": [
    "# QLoRA Fine-Tuning Llama-2 LLM\n",
    "* Notebook by Adam Lang\n",
    "* Date: 1/21/2025\n",
    "\n",
    "# Overview\n",
    "* In this notebook we will experiment and implement a QLoRA fine-tuning method using the Llama-2 model.\n",
    "\n",
    "# Fine-Tuning Method\n",
    "* We will utilize the method described in this paper: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
    "\n",
    "* QLORA introduces multiple innovations designed to reduce memory use without sacrificing performance:\n",
    "   1. `4-bit NormalFloat - \"NF-4\"`\n",
    "      * This is an optimal quantization data type for normally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.\n",
    "   2. `Double Quantization`\n",
    "      * A technique that quantizes the quantization constants, saving an average of about 0.37 bits per parameter (approximately 3 GB for a 65B model)\n",
    "   3. `Paged Optimizersusing`\n",
    "      * This technique avoids gradient neural network checkpointing memory spikes that occur when processing a mini-batch with a long sequence length.\n",
    "     \n",
    "## Fine-Tuning Considerations\n",
    "* You obviously need access to a GPU to do this. Whether it is through Google Colab or AWS SageMaker or another cloud instance or a local GPU.\n",
    "* The amount of memory usage that you will use when implementing:\n",
    "      1. optimizers\n",
    "      2. gradients (e.g. accumulations)\n",
    "      3. forward activation functions\n",
    "* Consider that FULL FINE-TUNING is NOT POSSIBLE as it is memory intensive and you can get nearly the same result using QLoRA or LoRA which are PEFT (parameter efficient fine-tuning) methods.\n",
    "* In order to reduce your VRAM usage, this is why we would use a technique like QLoRA which trains/fine-tunes the model in 4-bit precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f10ca-38b9-4d4d-ba77-50b2d46cb514",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a0a32f-67e7-432e-a7b2-4d8e3920d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50ae05-1744-49eb-8443-a2ee67e3a549",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57d75c8-ceb1-4ba8-a859-0331e099a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard DS Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "## ML imports\n",
    "import torch\n",
    "from datasets import load_dataset ## HF datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13e1b63-b2c8-440f-b034-71fed51cbee3",
   "metadata": {},
   "source": [
    "# Llama-2 Prompt Template for Chat Models\n",
    "* This is the template we need to use for fine-tuning a chat model.\n",
    "* The Llama-2 templates are found at this link from Meta: https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-2/\n",
    "\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "System prompt\n",
    "<</SYS>>\n",
    "\n",
    "User prompt [/INST] Model answer </s>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df99f3f-cf6c-4966-ba12-c957f58116c8",
   "metadata": {},
   "source": [
    "# Dataset and Formatting\n",
    "* The dataset we will use to fine-tune the model is a subset of the Open Assistant dataset from Hugging Face called the `timdettmers/openassistant-guanaco`.\n",
    "* Dataset card: https://huggingface.co/datasets/timdettmers/openassistant-guanaco\n",
    "* However, as with any fine-tuning task, we need to re-format the dataset to align with the format the model expects. Thus we need to re-format the \"Human\" and \"Assistant\" format to align with the Llama-2 prompt template above.\n",
    "* The dataset is also available open source via huggingface: https://huggingface.co/datasets/gpjt/openassistant-guanaco-llama2-format\n",
    "\n",
    "## Manually creating a Llama-2 dataset\n",
    "* You can use the dataset above or create it yourself using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f57162a1-c0d5-4d64-8917-5fe06d094da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration timdettmers--openassistant-guanaco-c21e85fd8b1a6952\n",
      "Reusing dataset json (/home/sagemaker-user/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-c21e85fd8b1a6952/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9e973e30f34918a36edf15b36cf1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/sagemaker-user/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-c21e85fd8b1a6952/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-b60c7806cd24aa5d.arrow\n"
     ]
    }
   ],
   "source": [
    "#from datasets import load_dataset\n",
    "#import re \n",
    "\n",
    "# load original hf dataset\n",
    "dataset = load_dataset('timdettmers/openassistant-guanaco')\n",
    "\n",
    "## shuffle and slice dataset\n",
    "dataset = dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "## function to transform dataset\n",
    "def transform_convo(source_text):\n",
    "    \"\"\"Function to transform conversational text into Llama-2 format\"\"\"\n",
    "    convo_text = source_text['text']\n",
    "    segments = convo_text.split('###')\n",
    "\n",
    "    ## store formatted text in list\n",
    "    reformatted_segments = []\n",
    "\n",
    "    ## iterate over pairs of segments\n",
    "    for i in range(1, len(segments) - 1, 2):\n",
    "        human_text = segments[i].strip().replace('Human:', '').strip()\n",
    "\n",
    "        ## Check if there is corresponding assistant segment before processing\n",
    "        if i + 1 < len(segments):\n",
    "            assistant_text = segments[i+1].strip().replace('Assistant:', '').strip()\n",
    "\n",
    "            # Apply new prompt template from Llama-2\n",
    "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] {assistant_text} </s>')\n",
    "        else:\n",
    "            # handle case where there is no corresponding assistant segment\n",
    "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] </s>')\n",
    "\n",
    "    return {'text': ''.join(reformatted_segments)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d06c1c2-2164-4b13-b5e1-c37dc7cfdd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134f92acf663433f8ef2cd3e82be5aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Apply transformation function using `.map` function from hugging face\n",
    "transformed_data = dataset.map(transform_convo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef3515-b350-4073-854f-af78333e3ed8",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "1. Load `llama-2-7b-chat-hf` chat llm model from hugging face.\n",
    "2. Train the model using ~1,000 samples from the guanaco dataset but in the llama-2 prompt template format. This was the original dataset used in the QLoRA paper.\n",
    "3. We will use these parameters:\n",
    "   * `Rank = 64`\n",
    "   * `Alpha = 16`\n",
    "\n",
    "* We are using the parameters above because we are taking a 32-bit model and coverting it to 4-bit quantized normal-float. \n",
    "\n",
    "# How to choose LoRA parameters?\n",
    "* Note: I bring this blurb with me everytime I fine-tune using PEFT as it is very helpful to remember the mathematical concepts at play here.\n",
    "\n",
    "1. **Rank (r)**\n",
    "* There is not \"magic number\" for LoRA, but most people go off the orignal LoRA paper which used r=8 and works for most problems and might be called the \"sweet spot\".\n",
    "\n",
    "Two things to remember:\n",
    "\n",
    "    * 1) If your dataset is significantly different and more complex compared to the dataset on which the model was pretrained, then it is best practice to use a HIGH rank value: `e.g. 64–256`\n",
    "    * 2) If the problem you’re adapting a pre-trained model to, is relatively simple and doesn’t involve a complex new dataset that the model hasn’t encountered before, it is best practice to use LOWER rank values: `e.g. 4-12`\n",
    "\n",
    "2. **Alpha (a)**\n",
    "* General rule of thumb about alpha:\n",
    "\n",
    "  1) HIGHER “alpha” would place more emphasis on the low-rank structure or regularization\n",
    "  2) LOWER “alpha” would reduce its influence, making the model rely more on the original parameters.\n",
    "\n",
    "* Adjusting “alpha” helps in finding a balance between fitting the data and preventing overfitting by regularizing the model.\n",
    "\n",
    "* How do we decide a good Alpha for your problem?\n",
    "    * Usually we choose an **alpha value that is twice as large as the rank** when fine-tuning LLMs (note that this is different when working with diffusion models).\n",
    "\n",
    "* In the original LoRA paper, the authors use `α=16` for their experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04f36a-028c-40d4-9afd-6cb888bdfd44",
   "metadata": {},
   "source": [
    "# Load Model, Dataset, and QLoRA parameters\n",
    "\n",
    "## 1. Setup fine-tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a2bea49-752d-4627-b4b7-750d54a4fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Llama model\n",
    "model_ckpt = 'NousResearch/Llama-2-7b-chat-hf'\n",
    "\n",
    "\n",
    "## 2. Instruction Dataset for fine-tuning\n",
    "## even though we transformed the dataset above for exercise purposes, we will use the dataset with llama format\n",
    "## direct from hugging face\n",
    "dataset_name = 'mlabonne/guanaco-llama2-1k'\n",
    "\n",
    "## 3. After fine-tuning the model the new name will be this below\n",
    "new_model = \"Llama-2-7b-chat-finetune\"\n",
    "\n",
    "######################################################\n",
    "# QLoRA Parmeters for fine-tuning\n",
    "\n",
    "# LoRA attention dim (matrix rank 'r')\n",
    "lora_r = 64\n",
    "\n",
    "# alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# dropout probability for LoRA model layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "#####################################################\n",
    "# bitsandbytes parameters\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit=True\n",
    "\n",
    "# compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype=\"float16\"\n",
    "\n",
    "# quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type=\"nf4\" ## 4-bit normal float\n",
    "\n",
    "# activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant=False\n",
    "\n",
    "#####################################################\n",
    "# TrainingArguments Parameters\n",
    "\n",
    "# 1. output directory for model preds and checkpoints\n",
    "output_dir = \"./model_results\"\n",
    "\n",
    "# 2. number of EPOCHS to train\n",
    "num_train_epochs = 1\n",
    "\n",
    "# 3. enable fp16/bf16 training (setting bf16 to True with A100 GPU) -- bfloat is brain floating point\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# 4. batch size per GPU for training LLM\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# 5. batch size per GPU for evaluating LLM\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# 6. Number of update steps to accumulate gradients after each forward pass\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# 7. Enable gradient checkpoints\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# 8. Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm=0.3\n",
    "\n",
    "# 9. learning rate (AdamW optimizer usually for fine-tuning)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# 10. weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# 11. Optimizer for fine-tuning\n",
    "optim = \"paged_adamw_32bit\" ## specific for QLoRA: https://github.com/artidoro/qlora\n",
    "\n",
    "# 12. learning rate schedule\n",
    "lr_scheduler_type = \"cosine\" ## cosine annealing with cosine curve for smooth decay and warm restart\n",
    "\n",
    "# 13. num of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# 14. ratio of steps for a linear warmup (0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "## Group sequences into batches with same length\n",
    "## Saves memory and speeds up training!!\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X update steps\n",
    "save_steps = 0\n",
    "\n",
    "# log every X update steps\n",
    "logging_steps = 25\n",
    "\n",
    "#####################################################################\n",
    "# Supervised Fine-Tuning (SFT) Parameters\n",
    "\n",
    "# Max sequence length to use \n",
    "max_seq_length = None\n",
    "\n",
    "# pack multiple short examples in same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# load entire model on GPU 0\n",
    "device_map = {\"\": 0} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d6ccc-c31e-4886-b899-9eb2a5b9bf59",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "* As I mentioned above, the dataset I loaded was direct from hugging face but if it were NOT already formatted, we would want to do the following here:\n",
    "      1) Reformat prompt for fine-tuning based on model demands.\n",
    "      2) Remove duplicated text or other miscellaneous data wrangling.\n",
    "      3) ....other data wrangling as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c388a27-55c2-4562-8b1c-76404b239870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration mlabonne--guanaco-llama2-1k-f1f1134768f90029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/mlabonne--guanaco-llama2-1k to /home/sagemaker-user/.cache/huggingface/datasets/mlabonne___parquet/mlabonne--guanaco-llama2-1k-f1f1134768f90029/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c849de5067f0482abec4b5e01f5f985b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d0b0a8d9ac4b68b6230952db435840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/967k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab22842155343079d7e2e4a28d35784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/sagemaker-user/.cache/huggingface/datasets/mlabonne___parquet/mlabonne--guanaco-llama2-1k-f1f1134768f90029/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "## load dataset\n",
    "dataset = load_dataset(dataset_name, split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0ba3a-ef85-4eb8-945c-9a1825feedf2",
   "metadata": {},
   "source": [
    "## 3. Configure QLoRa via bitsandbytes\n",
    "* 4-bit precision is configured here which is what makes this QLoRa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52987294-c82f-4bb8-88d7-6337e1b096d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load tokenizer and model into QLoRA config\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "## checking GPU compatability with bfloat16\n",
    "if compute_dtype == torch.bfloat16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b8287-ad7e-4e93-8249-0b8ec04be0b7",
   "metadata": {},
   "source": [
    "## 4. Load Base Model and Tokenizer from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "211e06c5-390b-4572-aec8-adc1d8c0a0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5076a92ef24875bd2557e2508cea88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac175faf2a1748d4bab0954ef0f18420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e660df7d6241788ab96ea3eb71e7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001b6929f12948e8bfbdec2caf36f00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9357a913d504b2ab09318c827e74c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caeb941ed9f242ea94741b8a9fb42571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e6ec3148ba484888b8f0038b2b2a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed10c066f0474342ab0acda0f5742ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bacc8a9c734950b5d02e2545c7d4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3756f51daf8448b3994aeb38602cecae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4646732b25416199dbda3995cea058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c63f484b1c44379c57771ce0280907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ckpt,\n",
    "    quantization_config=bnb_config, \n",
    "    device_map=device_map, ## map GPU\n",
    ")\n",
    "## set model configs\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1 ## need to set to 1 for parallel processing tensors on GPU\n",
    "\n",
    "\n",
    "## load model tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # set pad_token = to end of statement token\n",
    "tokenizer.padding_side = \"right\" ## fix overflow issue with fp16 training --> also pad right when using CausalLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56076f09-d85b-42c0-8b2a-4928686a1b05",
   "metadata": {},
   "source": [
    "## 5. Load LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50534461-3f63-4daa-9241-0294f1c00a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha, ## alpha params\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r, #rank of matrix\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" ## generative outputs for decoder model -- set to \"MASKED_LM\" if using encoder model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5630c89-81fc-4fd8-834a-b9857b8adbc9",
   "metadata": {},
   "source": [
    "## 6. Setup Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b8f056c-d57d-4354-adc7-bf41037ed280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5308076c0a484fda8fc853326d3b2532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## train params\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim, ## optimizer\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "\n",
    ")\n",
    "\n",
    "## set supervised fine-tuning params\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=packing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d90da-4fc2-42b9-a4b4-d76a98699a16",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07cd5a92-b730-4a1a-bd81-60820b3aa917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 09:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.444500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.541900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.3599420700073241, metrics={'train_runtime': 588.9108, 'train_samples_per_second': 1.698, 'train_steps_per_second': 0.425, 'total_flos': 8755214190673920.0, 'train_loss': 1.3599420700073241, 'epoch': 1.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train the model!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b4964-38ab-43d7-872a-1da6794ef83d",
   "metadata": {},
   "source": [
    "## 8. Save Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4580c1bb-fff1-46fc-9c11-b84efc65246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fcb555-cfae-4ffa-835c-812267fe3b99",
   "metadata": {},
   "source": [
    "## 9. Tensorboard\n",
    "* Here we can view:\n",
    "\n",
    "  1. Tracking and visualizing metrics such as loss and accuracy\n",
    "  2. Visualizing the model graph (ops and layers)\n",
    "  3. Viewing histograms of weights, biases, or other tensors as they change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ccd30b6-4b84-4fc4-acc7-56e592b96186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad39fa-003e-4cbb-9f54-15b875b95a77",
   "metadata": {},
   "source": [
    "# Inference on Trained Model\n",
    "* The guanaco dataset includes multiple languages as follows:\n",
    "```\n",
    "Japanese (Ja-JP - recently updated) 7,485 entries.\n",
    "Simplified Chinese (zh-Hans): 5,439 entries.\n",
    "Traditional Chinese (Taiwan) (zh-Hant-TW): 9,322 entries.\n",
    "Traditional Chinese (Hong Kong) (zh-Hant-HK): 9,954 entries.\n",
    "English: 20,024 entries, not from Alpaca.\n",
    "```\n",
    "* Japanese was one of the languages so i used a prompt in english it is: \"What is the difference between Cats and Dogs?\" but I translated it to japanese to ask the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a56dd799-7c21-4df1-a1c1-851ae1330ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] 猫と犬の違いは何ですか? [/INST] 猫と犬は、生物学的には同一の生物であるが、生活様式や行動などの面では異なる特徴を持つ。具体的には以下のような違いがある。\n",
      "\n",
      "1. 構造: 猫は、犬よりも小さな体を持つ。犬は、猫よりも大きな体を持つ。\n",
      "2. 毛色: 猫は、犬よりも多様な毛色を持つ。犬は、主に毛色が白色や斑点がある。\n",
      "3. 毛質: 猫は、犬よりも柔らかい毛を持つ。犬は、猫よりも硬い毛を持つ。\n",
      "4. 体重: 猫は、犬よりも軽い体重を持つ。犬は、猫よりも重い体重を持つ。\n",
      "5. 生活様式: 猫は、主に寝ている生活様式を持つ。犬は、主に活動的な生活様式を持つ。\n",
      "6. 食事: 猫は、主に魚や肉を食べる。犬\n"
     ]
    }
   ],
   "source": [
    "## ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# run text generation hf pipeline with new model -- japanese prompt\n",
    "prompt = \"猫と犬の違いは何ですか?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=400)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\") ## llama-2 prompt template\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63162dca-f511-4e0e-907e-65ce93ceb2f7",
   "metadata": {},
   "source": [
    "# Use Llama to translate the result back to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbd4b698-4e61-467b-b4f3-1fcb432d1cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Can you translate this \n",
      "1. 構造: 猫は、犬よりも小さな体を持つ。犬は、猫よりも大きな体を持つ。\n",
      "2. 毛色: 猫は、犬よりも多様な毛色を持つ。犬は、主に毛色が白色や斑点がある。\n",
      "3. 毛質: 猫は、犬よりも柔らかい毛を持つ。犬は、猫よりも硬い毛を持つ。\n",
      "4. 体重: 猫は、犬よりも軽い体重を持つ。犬は、猫よりも重い体重を持つ。\n",
      "5. 生活様式: 猫は、主に寝ている生活様式を持つ。犬は、主に活動的な生活様式を持つ。\n",
      "6. 食事: 猫は、主に魚や肉を食べる。犬\n",
      " from japanese to english? [/INST] Sure, here are the translations of the six points from Japanese to English:\n",
      "\n",
      "1. 構造: 猫は、犬よりも小さな体を持つ。犬は、猫よりも大きな体を持つ。\n",
      "\n",
      "Translation: Structure: Cats have smaller bodies than dogs. Dogs have larger bodies than cats.\n",
      "\n",
      "2. 毛色: 猫は、犬よりも多様な毛色を持つ。犬は、主に毛色が白色や斑点がある。\n",
      "\n",
      "Translation: Hair color: Cats have more diverse hair colors than dogs. Dogs are mainly white or have spots.\n",
      "\n",
      "3. 毛質: 猫は、犬よりも柔らかい毛を持つ。犬は、猫よりも硬い毛を持つ。\n",
      "\n",
      "Translation: Hair quality: Cats have softer hair than dogs. Dogs have harder hair than cats.\n",
      "\n",
      "4. 体重: 猫は、犬よ\n"
     ]
    }
   ],
   "source": [
    "# run text generation hf pipeline with new model -- japanese prompt\n",
    "translate_text = \"\"\"\n",
    "1. 構造: 猫は、犬よりも小さな体を持つ。犬は、猫よりも大きな体を持つ。\n",
    "2. 毛色: 猫は、犬よりも多様な毛色を持つ。犬は、主に毛色が白色や斑点がある。\n",
    "3. 毛質: 猫は、犬よりも柔らかい毛を持つ。犬は、猫よりも硬い毛を持つ。\n",
    "4. 体重: 猫は、犬よりも軽い体重を持つ。犬は、猫よりも重い体重を持つ。\n",
    "5. 生活様式: 猫は、主に寝ている生活様式を持つ。犬は、主に活動的な生活様式を持つ。\n",
    "6. 食事: 猫は、主に魚や肉を食べる。犬\n",
    "\"\"\"\n",
    "prompt = f\"Can you translate this {translate_text} from japanese to english?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=600)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\") ## llama-2 prompt template\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7729c4e8-b473-4049-8887-229393e04e35",
   "metadata": {},
   "source": [
    "* If we compare google translates results:\n",
    "```\n",
    "Structure: Cats have smaller bodies than dogs. Dogs have larger bodies than cats.\n",
    "2. Coat Color: Cats have a wider variety of coat colors than dogs. Dogs mainly have white or spotted coats.\n",
    "3. Coat quality: Cats have softer coats than dogs. Dogs have harder fur than cats.\n",
    "4. Weight: Cats weigh less than dogs. Dogs weigh more than cats.\n",
    "5. Lifestyle: Cats have a primarily sleeping lifestyle. Dogs have a mainly active lifestyle.\n",
    "6. Diet: Cats mainly eat fish and meat. dog\n",
    "```\n",
    "While not 100% the same it was very similar. Certainly we can try other multi-lingual examples and fine-tune on more specific examples to make the model better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812e90e-433c-4c12-ab94-17e91e06165c",
   "metadata": {},
   "source": [
    "# Final Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b099cd32-d6f3-4b0e-a27d-482da16f91a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc ## garbage collection\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f1017-afb8-4fee-a636-bb6a76dfd1bc",
   "metadata": {},
   "source": [
    "# Push model to HF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "426c851a-7f7f-4de4-ae0c-1e5e6014fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c123a1-7ffa-4d60-ae68-51f9d9cd9df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867fc72-2296-4141-b1cf-30bc92e59220",
   "metadata": {},
   "outputs": [],
   "source": [
    "## push to hub\n",
    "#model.push_to_hub(\"adamNLP/Llama-2-7b-chat-finetune\",check_pr=True)\n",
    "\n",
    "#tokenizer.push_to_hub(\"adamNLP/Llama-2-7b-chat-finetune\", check_pr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218078e1-7280-46cc-acaa-c4862ce0bef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
