{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4586dd1-2643-48c5-82a6-aeb9ecfb0dff",
   "metadata": {},
   "source": [
    "# Text Classification on Twitter Complaints Dataset using Prompt Tuning on Mistral-7B\n",
    "* Notebook by Adam Lang\n",
    "* Date: 12/16/2024\n",
    "\n",
    "# Overview\n",
    "* In this notebook we will perform text classification on a twitter complaints dataset using an LLM by leveraging the power of PEFT methods specifically \"Prompt Tuning\" which is a sub category of Prompt Learning PEFT techniques.\n",
    "\n",
    "# LLM we are prompt tuning\n",
    "* We will perform prompt tuning on the Mistral-7B model.\n",
    "* Here is the model card: https://huggingface.co/mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5cb5e-854c-4da8-acef-f76b19dde4d2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bfecaae-9400-4025-847f-995f0059b151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madam-m-lang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20241216_182438-mk7iulss</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adam-m-lang/prompt_learning_methods/runs/mk7iulss' target=\"_blank\">prompt_tuning</a></strong> to <a href='https://wandb.ai/adam-m-lang/prompt_learning_methods' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adam-m-lang/prompt_learning_methods' target=\"_blank\">https://wandb.ai/adam-m-lang/prompt_learning_methods</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adam-m-lang/prompt_learning_methods/runs/mk7iulss' target=\"_blank\">https://wandb.ai/adam-m-lang/prompt_learning_methods/runs/mk7iulss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "## setttings\n",
    "wandb.init(project=\"prompt_learning_methods\", name=\"prompt_tuning\")\n",
    "seed = 42\n",
    "device = \"cuda\"\n",
    "## load model and tokenizer from HF\n",
    "model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "## datset inormation\n",
    "dataset_name = \"twitter_complaints\"\n",
    "text_column = \"Tweet text\"\n",
    "label_column = \"text_label\"\n",
    "\n",
    "## other settings\n",
    "max_length = 64 ## max length of a tweet we will classify\n",
    "lr = 1e-4\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e6881-af51-42d8-9004-dd375c1aaaf3",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "* We first need to prepare the dataset.\n",
    "\n",
    "## Load the dataset from hugging face\n",
    "* https://huggingface.co/davidschulte/ESM_ought__raft_twitter_complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becfb8bc-49a7-4453-a6fa-58643a552adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unlabeled', 'complaint', 'no complaint']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3a2e1532a84c8c9643e22517ebfa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a691bbdfb7684a809c0fd648be88525f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Tweet text', 'ID', 'Label', 'text_label'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Tweet text', 'ID', 'Label', 'text_label'],\n",
      "        num_rows: 3399\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Tweet text': '@HMRCcustomers No this is my first job',\n",
       " 'ID': 0,\n",
       " 'Label': 2,\n",
       " 'text_label': 'no complaint'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "## load dataset\n",
    "dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "\n",
    "## setup train split\n",
    "classes = [k.replace(\"_\", \" \") for k in dataset['train'].features['Label'].names]\n",
    "print(classes)\n",
    "\n",
    "## map to dataset\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "## print dataset and first index\n",
    "print(dataset)\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d11f935-96f7-4e66-a6d8-538966d70314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 33, 1: 17})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets count the train labels\n",
    "from collections import Counter \n",
    "\n",
    "Counter(dataset[\"train\"][\"Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3054310-1702-4d79-bee6-82e73fc9d232",
   "metadata": {},
   "source": [
    "Summary\n",
    "* As with most classification tasks we can see the target class is skewed.\n",
    "* \"1\" is a complaint with 17 total\n",
    "* \"2\" is non-complaint with about twice as many labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c97cf-db14-46fe-a9a6-d33f0ab85d74",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86f9ea02-97ce-4853-922c-2ac8d01d8068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e4676e4a6d4319b43f0d53edcb7fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40474d3adbf04b10902a26d8dcc8641e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7a829f08f54b25870883eb8477b46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4deb5eccbc7547348cd53374d53f20dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_max_length=4\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer for Mistral-7B model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path) # add hf_token if not using cli\n",
    "## if text doesn't have pad token --> add eos token\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# get max length of tokens\n",
    "target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n",
    "print(f\"{target_max_length=}\")\n",
    "\n",
    "\n",
    "## data preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    #1. get batch_size\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x}\\nLabel : \" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1a193-a25e-4802-a49e-a903d585897b",
   "metadata": {},
   "source": [
    "Apply preprocessing function to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5128374-4149-4a76-aa74-29b065f4b6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf548f8876304975be49cd8bbb026d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0c23a3ba8649868d312ac21bc60543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     1,   320, 12523,  2245,   714,   802,   642, 28711,   311,\n",
       "           8503,  1080,  6304,   272,  9827,   354,   528,    13,  4565,   714,\n",
       "          28705,   708, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     1,   320, 12523,  2245,\n",
       "            714,   802,  2707,  6024, 28754,   525, 28709, 28743,  4585, 15359,\n",
       "           8196,   354,  1558,  4089, 28725,   829,   347,   586,  7865,   562,\n",
       "           3062,  2368, 28742, 28707,  1709,   298,   347,  2739,    13,  4565,\n",
       "            714, 28705, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     1,   320, 12523,  2245,   714,   802,  9979,  1242,\n",
       "           8062,   802,   675,  2867,   399,  2665,   528,    13,  4565,   714,\n",
       "          28705,   708, 22105,     2],\n",
       "         [    2,     2,     2,     1,   320, 12523,  2245,   714,  1450,  3231,\n",
       "            288, 28808, 23292, 28705, 28750, 10712,  6434,   354, 19338,   297,\n",
       "            422,  6263, 28708,   294,  4120, 28725,   422, 28759, 28798,   422,\n",
       "           6487,   374,   380,   422, 11966,   274, 28722,   734,   883,  6043,\n",
       "          14716, 28808, 28878,  4449,  1508, 28707, 28723,  1115, 28748,  6042,\n",
       "          28781,  2228, 28758, 28814, 28729, 28755, 28729,    13,  4565,   714,\n",
       "          28705,   708, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     1,   320, 12523,  2245,   714,   802,   811,\n",
       "          28709, 28730, 11538,   802,  4888,  1536,   574, 12688,   349,  9783,\n",
       "          18000,   349,  2115,   304,  1149,  1012,  4308,   290,  1126, 28723,\n",
       "           2483,   378,  2553,   378, 28742, 28713,  1318,  4876,    13,  4565,\n",
       "            714, 28705, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     1,   320, 12523,  2245,\n",
       "            714,   802, 28765,   279,  2581,  9452,   739,   460,   368,  8817,\n",
       "            288,   633,  9917, 10190,   354,  6735,  2668,    13,  4565,   714,\n",
       "          28705,   708, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     1,   320, 12523,  2245,   714,  3194,\n",
       "          10198,   264,  8428,  4449,  1508, 28707, 28723,  1115, 28748, 28754,\n",
       "           1981, 28765, 28727, 28743, 28768,  7209, 28718,    13,  4565,   714,\n",
       "          28705,   708, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     1,   320,\n",
       "          12523,  2245,   714, 12376,  2815,  3500, 19521, 28725,   579, 17949,\n",
       "          28723, 15108,   422, 21404,  2199,  3167, 14265,    13,  4565,   714,\n",
       "          28705,   708, 22105,     2]]),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   708, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   708, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   708, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   708, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   708, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   708, 22105,     2]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## map function to dataset\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "## set train and validation data -- same for simplicity \n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"train\"]\n",
    "\n",
    "\n",
    "\n",
    "## setup dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f3095-4354-4507-9b77-8def669f2f3a",
   "metadata": {},
   "source": [
    "Summary\n",
    "* If you see -100 it means the token is ignored. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0963becf-d212-45bd-88e9-5d8236d2e70d",
   "metadata": {},
   "source": [
    "## Preprocess Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3c8bc6e-537b-4c45-889a-2542d9b845b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b78c1515b8548a7991311e96aaffa8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     1,   320, 12523,  2245,   714,   802, 28760,   324, 14233,\n",
       "           2328,  8868,   354, 10313,   586,  7416,  2169,   395,   272,  4908,\n",
       "           8147,  1309,   356,   378, 28808, 11936, 28723, 22747, 28723,   675,\n",
       "          28748, 28710, 28737, 28734, 11788, 28744, 28762, 28779, 28779, 28750,\n",
       "             13,  4565,   714, 28705],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     1,   320, 12523,  2245,\n",
       "            714,   334,   855,   288,   582,   356,   422,  3836,  3957,  1829,\n",
       "          19653, 28808,   415,   905,   590,  3088,   354,  1167,  4370,   568,\n",
       "           1371,   272,  8710,   472, 28705, 29137, 29137, 29274, 30155, 29096,\n",
       "             13,  4565,   714, 28705],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     1,   320, 12523,  2245,   714, 15359,   802, 21270, 28713,\n",
       "          17302, 28725,   349,   378,  1055, 20775, 28713,  4920,   298,   865,\n",
       "           2405,   264, 20106,   513,   272,  2515,   349,  9683, 28724, 28804,\n",
       "             13,  4565,   714, 28705],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              1,   320, 12523,  2245,   714,   802, 28798,  9399, 24855,  5156,\n",
       "            315, 28809, 28719, 13903,  1101,   335,   272,  1537, 28735,   349,\n",
       "            272,   981,   291,   529, 17381,  3372,   511,   368,  3091,   369,\n",
       "           8623, 28733,  8820,  6835,   460, 17381, 28878,  4449,  1508, 28707,\n",
       "          28723,  1115, 28748,   299, 17580, 28728, 28754,  2252, 28740, 28716,\n",
       "             13,  4565,   714, 28705],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     1,   320, 12523,\n",
       "           2245,   714,   802, 13052,   795,   602,  7261,  6504,  8196,  1101,\n",
       "             13,  4565,   714, 28705],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     1,   320, 12523,  2245,   714,   802, 28719,   641,\n",
       "            301,   463,  9255,   490, 28706,   613, 28809, 28719,  2590, 15671,\n",
       "             13,  4565,   714, 28705],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     1,\n",
       "            320, 12523,  2245,   714,   802, 28757, 14595, 28735,  3851,   298,\n",
       "           3848,   456,  3154, 28723, 28301,   272, 24942,   773, 28705, 28781,\n",
       "          28734, 28823,   805,   954, 24104, 28723,  7336,  2240,   295,  1449,\n",
       "            460,   459,  4525,   345,   763, 24104, 28739,  4449,  1508, 28707,\n",
       "          28723,  1115, 28748, 28728,   510, 16457, 28740, 28765,  1737, 28787,\n",
       "             13,  4565,   714, 28705],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     1,   320,\n",
       "          12523,  2245,   714,   802, 28719,  3865,   322,   802, 28719,  3810,\n",
       "            444, 28721,  9050,   851,   349,   272,  1080,  7714,  8710,  1970,\n",
       "            315, 28742,   333,  2270,  3364,   302, 28723,  8580,  3707,   349,\n",
       "           1038,  2525,  7656,  4564,  2117, 28878,  4449,  1508, 28707, 28723,\n",
       "           1115, 28748, 28719, 28783, 28715,  2547, 28734,   278, 28779, 28726,\n",
       "             13,  4565,   714, 28705]]),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## function to preprocess test dataset\n",
    "def test_preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x}\\nLabel : \" for x in examples[text_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "## map function to test data\n",
    "test_dataset = dataset[\"test\"].map(\n",
    "    test_preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "## setup test data loader\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6021a04-4b90-4db4-8ebc-9c2e9a235877",
   "metadata": {},
   "source": [
    "# Create PEFT Model, Optimizer and Learning Rate Scheduler\n",
    "\n",
    "## Prompt Tuning Config\n",
    "* We need to setup the prompt tuning config first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743ce6dd-2853-4310-bd5e-834015fade9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first you need the prompt instructing the model to classify tweets\n",
    "prompt_tuning_init_text=\"Classify if the tweet is a complaint or no complaint.\\n\"\n",
    "\n",
    "## config -- init soft prompt token with embeddings for the prompt above\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, #predicting next word as complaint vs. no complaint\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT, ## instruction prompt to classify text\n",
    "    num_virtual_tokens=len(tokenizer(prompt_tuning_init_text)[\"input_ids\"]),\n",
    "    prompt_tuning_init_text=prompt_tuning_init_text,\n",
    "    tokenizer_name_or_path=model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db513de-44dd-4f35-a044-ddbb5be79a1b",
   "metadata": {},
   "source": [
    "## Create Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd6cb9fb-8b8b-46e4-a566-428ad96529a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc25e8239e294317866e46b980d85ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf25100ed8e4e2b8a7188ff94716c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c36d75726f9471986077b83a785505c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0448d0ec0e714fee927bedbb709c34f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a77fb3649b467ea003745aa92befac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dac065ab0e74e348db086e10b6f4a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0f787933a5429e83f2da1b6e0a170a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 61,440 || all params: 7,241,793,536 || trainable%: 0.0008\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16) #load in half-precision\n",
    "model = get_peft_model(model, peft_config) ##base model and peft_config\n",
    "model.print_trainable_parameters()\n",
    "## enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\":False})\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d10c18-d6e6-4edb-9c74-abad28322b1c",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* We can see the trainable params are only 61,440 compared to the full 7.2B params in the FULL Mistral model.\n",
    "* And that works out to be 0.008%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c893dd-dab8-4331-8190-9ba2b5f8c35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): MistralForCausalLM(\n",
       "    (model): MistralModel(\n",
       "      (embed_tokens): Embedding(32000, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MistralDecoderLayer(\n",
       "          (self_attn): MistralSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEmbedding(\n",
       "      (embedding): Embedding(15, 4096)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(32000, 4096)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lets see what the model parameters are\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8df66e-18b6-46b7-bc4d-6e4a8a37754f",
   "metadata": {},
   "source": [
    "Summary\n",
    "* We can see the `prompt_encoder` above has 15 embeddings which are for the input prompt which will be part of the soft prompt finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a6c15-d944-4468-aee1-f8a32e45d258",
   "metadata": {},
   "source": [
    "## Optimizer and Learning Rate Scheduler\n",
    "* Here just as with any deep learning model, we setup the optimizer and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80af10a9-5620-4a54-89a6-2773fcf4b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "\n",
    "## learning_rate scheduler\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0, #no warm up due to only updating subset of weights\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8f0dd-b283-4d32-9da8-d1f1e13a3060",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation on Test Samples BEFORE finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5dfd0f1-fed6-47b0-98ff-f83c965eeb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:1889: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text : @TommyHilfiger Dramatic shopping exp. ordered 6 jeans same size (30/32) 2 fits / 2 too large / 2 too slim : same brand &gt; different sizing\n",
      "Label : 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "i = 33 ## random index 33\n",
    "## tokenize input\n",
    "inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]}\\nLabel : ', return_tensors=\"pt\")\n",
    "\n",
    "## get prediction without gradients \n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=20, eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a18aaf-5ccf-4def-b718-72624559bf84",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* What we see above is 2 things:\n",
    "1. The label assigned to this tweet we are not even sure if that is correct or not.\n",
    "2. The model is continuing causal generation rather than classification. This should happen because the base model Mistral is not pretrained for classifying tweets it is a generative AI decoder model, hence why we are going to fine tune it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320945d-4e15-4ad1-a8da-c59e98f3be3b",
   "metadata": {},
   "source": [
    "# Training and Evaluation Loop\n",
    "* Here I am using a custom written PyTorch training and test loops rather than the Trainer API templates from hugging face.\n",
    "* This is the \"boiler plate code\" approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac895408-8d63-43a0-85dd-e95f210dd2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.32it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(1.0870, device='cuda:0') | train_epoch_loss=tensor(0.0834, device='cuda:0') eval_ppl=tensor(1.0709, device='cuda:0') | eval_epoch_loss=tensor(0.0685, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.35it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(1.0717, device='cuda:0') | train_epoch_loss=tensor(0.0692, device='cuda:0') eval_ppl=tensor(1.0709, device='cuda:0') | eval_epoch_loss=tensor(0.0685, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.38it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=tensor(1.0751, device='cuda:0') | train_epoch_loss=tensor(0.0724, device='cuda:0') eval_ppl=tensor(1.0709, device='cuda:0') | eval_epoch_loss=tensor(0.0685, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.36it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl=tensor(1.0929, device='cuda:0') | train_epoch_loss=tensor(0.0889, device='cuda:0') eval_ppl=tensor(1.0709, device='cuda:0') | eval_epoch_loss=tensor(0.0685, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.20it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl=tensor(1.0831, device='cuda:0') | train_epoch_loss=tensor(0.0798, device='cuda:0') eval_ppl=tensor(1.0709, device='cuda:0') | eval_epoch_loss=tensor(0.0685, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.29it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5: train_ppl=tensor(1.0709, device='cuda:0') | train_epoch_loss=tensor(0.0685, device='cuda:0') eval_ppl=tensor(1.0709, device='cuda:0') | eval_epoch_loss=tensor(0.0685, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.32it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6: train_ppl=tensor(1.0696, device='cuda:0') | train_epoch_loss=tensor(0.0673, device='cuda:0') eval_ppl=tensor(1.0709, device='cuda:0') | eval_epoch_loss=tensor(0.0685, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.32it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7: train_ppl=tensor(1.0713, device='cuda:0') | train_epoch_loss=tensor(0.0689, device='cuda:0') eval_ppl=tensor(1.0709, device='cuda:0') | eval_epoch_loss=tensor(0.0685, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.32it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8: train_ppl=tensor(1.0763, device='cuda:0') | train_epoch_loss=tensor(0.0735, device='cuda:0') eval_ppl=tensor(1.0709, device='cuda:0') | eval_epoch_loss=tensor(0.0685, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.09it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9: train_ppl=tensor(1.0698, device='cuda:0') | train_epoch_loss=tensor(0.0675, device='cuda:0') eval_ppl=tensor(1.0709, device='cuda:0') | eval_epoch_loss=tensor(0.0685, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training and evaluation\n",
    "for epoch in range(num_epochs):\n",
    "    ## 1. train\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ## 2. Forward Pass\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.autocast(dtype=torch.float16, device_type=\"cuda\"):\n",
    "            outputs = model(**batch)\n",
    "    ## 3. Calculate loss\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "    ## 4. Backpropagation\n",
    "        loss.backward()\n",
    "    ## 5. Optimizer step -- update model params after gradients computed\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "    ## 5. Zero out gradients -- avoid gradient accumulation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Testing loop\n",
    "    # 1. eval\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    ## 2. forward pass \n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "    ## 3. Calculate loss\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    ## print out whats happening!!\n",
    "    print(f\"{epoch=}: {train_ppl=} | {train_epoch_loss=} {eval_ppl=} | {eval_epoch_loss=}\")\n",
    "    wandb.log({\"train\": {\"perplexity\": train_ppl, \"loss\": train_epoch_loss, \"epoch\": epoch}, \n",
    "               \"val\": {\"perplexity\": eval_ppl, \"loss\": eval_epoch_loss, \"epoch\": epoch}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0f4be-e8d5-4f12-937f-20b33edb9732",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation on Test Samples AFTER Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5d74b29-f2a2-48f3-af5b-415555237d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Tweet text : @TommyHilfiger Dramatic shopping exp. ordered 6 jeans same size (30/32) 2 fits / 2 too large / 2 too slim : same brand &gt; different sizing\n",
      "Label :  complaint</s>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "i = 33\n",
    "inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]}\\nLabel : ', return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=5, eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76ce472-1be3-44d5-bde4-7ffa8f5d0fca",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Now let's compare the fine tuned model to the original tweet #33.\n",
    "* We can see that it has labeled the same tweet as a \"complaint\" which previously the model was not able to do. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e42da-8bcd-4ae9-962f-d247079fa568",
   "metadata": {},
   "source": [
    "# Option 1 - Save Model and Push to hugging face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01679a62-e249-4c91-a1ac-cd61153d2cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03afb723360470999d62adc85d146db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, logout\n",
    "\n",
    "logout() # Log out first\n",
    "notebook_login() # Then log in again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8659b49-81ae-4ef5-a32e-ae1c7d881ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\n",
    "#     f\"mistral_prompt_tuning\",\n",
    "#     token = \"hf_...\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # save our model to the Hugging Face hub\n",
    "# model_upload_url = model.push_to_hub(\n",
    "#     commit_message=\"Uploading PEFT Prompt Tuned Mistral-7B for Text Classification\",\n",
    "#     token = \"\",\n",
    "# )\n",
    "# # print if successful upload\n",
    "# print(f\"[INFO] Model successfully uploaded to the Hugging Face Hub with URL: {model_upload_url}\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5218b-8b3d-4f4b-a0b0-7436bdd49d48",
   "metadata": {},
   "source": [
    "# Option 2 - Save model local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acec1d76-e899-43b0-b1c4-386a1c7e4621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/adamNLP/mistral_prompt_tuning/commit/b6330b2d3089ac752a7190126716592d8355248e', commit_message='Upload model', commit_description='', oid='b6330b2d3089ac752a7190126716592d8355248e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/adamNLP/mistral_prompt_tuning', endpoint='https://huggingface.co', repo_type='model', repo_id='adamNLP/mistral_prompt_tuning'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving model\n",
    "peft_model_id = \"mistral_prompt_tuning\"\n",
    "model.push_to_hub(peft_model_id, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba811f89-7446-42cf-924b-80b72978a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 19:21:02 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  NVIDIA RTX A6000               On  |   00000000:29:00.0 Off |                  Off |\n",
      "| 30%   43C    P2             76W /  300W |   15042MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b44f8-f322-4e45-a87e-fb404a5f0ec4",
   "metadata": {},
   "source": [
    "# Load the model Checkpoint and Perform Qualitative Analysis on Test Samples\n",
    "* We can load it from our own huggingface space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d5b453c-d3dd-4262-8d8f-e7ae4e74c2af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef185d82519244e5ae1c073c484fe06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "## load same dataset we fine tuned on \n",
    "dataset = load_dataset(\"ought/raft\", \"twitter_complaints\")\n",
    "\n",
    "## load custom model that I finetuned from HF\n",
    "peft_model_id = \"adamNLP/mistral_prompt_tuning\"\n",
    "device = \"cuda\" ## put on GPU\n",
    "text_column = \"Tweet text\"\n",
    "label_column = \"text_label\"\n",
    "\n",
    "## load config\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "## load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf1f52-a4be-4891-b7d7-32ba6a085f05",
   "metadata": {},
   "source": [
    "## Run Test Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e6f3e-2ba8-4fce-9b89-7ab2efc34154",
   "metadata": {},
   "source": [
    "Test Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5c2a535-d207-4b59-931f-494e27cc53be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text : @virginmedia Instead of spending money on advertising, why not fix the slow speeds in the RG2 area. CLOWNS\n",
      "Label :  complaint\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "i = 36\n",
    "inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]}\\nLabel : ', return_tensors=\"pt\")\n",
    "# print(dataset[\"test\"][i][\"Tweet text\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089c308-2018-4a85-8541-829d4d971cbc",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* Clearly the model was able to classify this tweet as a complaint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec51abf-47d5-4de7-b942-7e423d299637",
   "metadata": {},
   "source": [
    "Test Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba110829-a2db-4689-9394-1f254eb71d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text : @NFLUK @Patriots That's right, #OnlyInTheNFL will the refs call an obvious TD incomplete and ruin a great game.\n",
      "Label :  no complaint\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "i = 22\n",
    "inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]}\\nLabel : ', return_tensors=\"pt\")\n",
    "# print(dataset[\"test\"][i][\"Tweet text\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda9e542-0fac-4dc5-97e3-41e663409787",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* Clearly this tweet is not a complaint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d5b6aef-6612-4b33-9eeb-ba17d55f7c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 19:26:53 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:29:00.0 Off |                  Off |\n",
      "| 30%   41C    P2             86W /  300W |   42210MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc8885-b079-49fa-8a5e-ae6c70409d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
